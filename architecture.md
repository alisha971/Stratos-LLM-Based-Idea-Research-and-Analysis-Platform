# System Architecture

## Overview
This project implements a distributed, event-driven AI system designed to generate structured, citation-backed outputs.  
The architecture emphasizes modularity, asynchronous execution, and real-time feedback while remaining extensible for future capabilities.

---

## Architectural Style
The system follows a **microservice-oriented architecture** built around:
- Asynchronous task execution
- Centralized orchestration
- Event-based progress streaming

This design enables scalability, fault isolation, and parallel processing across independent services.

---

## System Architecture Diagram

```mermaid
flowchart LR
    User[Client]
    API[API Gateway]
    Auth[Authentication]
    Orch[Orchestrator]

    Queue[Task Queue]
    Workers[Background Worker Services]
    Events[Event Stream]

    DB[(Relational Storage)]
    Vector[(Vector Store)]
    Files[(Export Storage)]

    LLM[LLM & Embedding Services]

    User --> API
    API --> Auth
    API --> Orch

    Orch --> Queue
    Queue --> Workers

    Workers --> DB
    Workers --> Vector
    Workers --> Files
    Workers --> LLM

    Workers --> Events
    Events --> API
    API --> User
````

---

## Client Layer

### Client Application

* Browser-based interface for initiating requests and consuming results
* Receives real-time progress updates via server-side streaming
* Maintains minimal local state

---

## API & Orchestration Layer

### API Gateway

**Technologies:** FastAPI, Server-Sent Events (SSE)

Responsibilities:

* Primary entry point for client requests
* Authentication and session validation
* Streaming execution updates to the client
* Request lifecycle coordination

---

### Authentication

**Technologies:** OAuth, JWT

Responsibilities:

* User identity verification
* Token-based access control
* Secure session management

---

### Orchestrator Service

The orchestrator coordinates request execution at a high level.

Responsibilities:

* Manages request lifecycle and state
* Decomposes requests into background tasks
* Dispatches work to worker services via the task queue
* Tracks execution metadata for recovery and observability

---

## Messaging & Event Infrastructure

### Task Queue

* Enables asynchronous and distributed execution
* Decouples request handling from compute-intensive processing
* Supports horizontal scaling of worker services

---

### Event Stream

* Publishes execution events generated by workers
* Enables real-time client updates
* Improves system observability without blocking execution

---

## Worker Services

Worker services are implemented as independent background processes, each focused on a single responsibility.

### Characteristics

* Stateless execution
* Horizontally scalable
* Triggered exclusively via the task queue
* Persist results to shared storage
* Emit progress and completion events

---

## Storage Layer

### Relational Storage

Used for:

* User sessions and request state
* Structured content artifacts
* Metadata and progress tracking

---

### Vector Store

Used for:

* Semantic embeddings
* Similarity-based retrieval
* Evidence and contextual data storage

---

### Export Storage

Used for:

* Generated reports and artifacts
* Persisted output formats such as documents or web exports

---

## AI & Embedding Services

### Language Model Services

Language models are utilized across worker services to support:

* Reasoning and synthesis
* Content generation
* Data transformation tasks

---

### Embedding Services

* Generate vector representations of structured and unstructured content
* Enable semantic search and retrieval workflows

---

## Data Flow Overview

1. A client request is received by the API gateway
2. The orchestrator coordinates background task execution
3. Worker services process tasks asynchronously
4. Results are persisted across storage layers
5. Progress and outputs are streamed back to the client
6. Final artifacts are assembled and made available for export

---

## Design Principles

* Asynchronous-first execution
* Loose coupling via messaging
* Clear separation of concerns
* Horizontal scalability
* Fault isolation at the service level

---

## Extensibility

The architecture is designed to support:

* Additional worker services
* Alternative model providers
* New storage backends
* Enhanced observability and monitoring

